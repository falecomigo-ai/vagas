# ğŸ§  Desafio TÃ©cnico â€“ Desenvolvedor Backend (AI Stack)

## ğŸ“ CenÃ¡rio Real

VocÃª foi contratado como **Desenvolvedor Backend** em uma empresa SaaS que oferece **assistentes de vendas inteligentes** para equipes comerciais via WhatsApp, e-mail e CRM.  
O produto utiliza **modelos de linguagem (LLMs)** para automatizar mensagens, responder clientes e gerar insights de performance.

O time quer que vocÃª lidere a criaÃ§Ã£o do **mÃ³dulo central de â€œIA conversacional com memÃ³ria e ferramentasâ€**, que permitirÃ¡ que os agentes de IA da plataforma sejam configurÃ¡veis e conectem-se a dados reais de cada cliente.

---

## ğŸ§© Parte 1 â€” AnÃ¡lise de Requisitos e Arquitetura

### ğŸ§¾ Requisitos Funcionais

1. O sistema deve receber mensagens de um usuÃ¡rio e gerar respostas usando uma API de LLM (OpenAI, Claude, Gemini, etc.);
2. O sistema deve manter **memÃ³ria da conversa** por usuÃ¡rio;
3. O sistema deve permitir a execuÃ§Ã£o de **aÃ§Ãµes externas (â€œferramentasâ€)**, como:
   - Buscar dados de vendas em um banco local;
   - Fazer chamadas a APIs externas (por exemplo, dados de mercado);
4. O sistema deve expor essas funcionalidades via uma **API REST ou WebSocket**;
5. O sistema deve registrar mÃ©tricas de uso (tokens, tempo de inferÃªncia, custo estimado);
6. Deve ser **containerizÃ¡vel (Docker)**.

### ğŸ§± Requisitos NÃ£o Funcionais

- CÃ³digo limpo e modular;
- Logs e rastreabilidade de execuÃ§Ã£o;
- EscalÃ¡vel horizontalmente (sem depender de estado em memÃ³ria local);
- ConfiguraÃ§Ã£o via `.env`;
- DocumentaÃ§Ã£o clara (`README.md` e `ARCHITECTURE.md`).

---

### ğŸ§  Sua primeira tarefa

Crie um arquivo chamado `ARCHITECTURE.md` com:

- DescriÃ§Ã£o da arquitetura proposta (pode incluir diagramas);
- Componentes principais (API Gateway, Core LLM Service, Memory Layer, Tool Executor, etc.);
- Justificativas tÃ©cnicas (por que escolher determinado framework, cache, mensageria, etc.);
- EstratÃ©gia de observabilidade e logs;
- Plano de escalabilidade (ex: workers, filas, cache distribuÃ­do);
- Modelo de dados resumido (entidades principais e suas relaÃ§Ãµes).

---

## âš™ï¸ Parte 2 â€” ImplementaÃ§Ã£o do MÃ³dulo de IA

Implemente um **MVP funcional** da arquitetura descrita.

### âœ… Funcionalidades mÃ­nimas

#### 1. Endpoint `/chat`

- **Entrada:**
  ```json
  {
    "user_id": "123",
    "message": "Quais foram as vendas do mÃªs passado?"
  }
  ```
- **Processo:**

  - Use uma API de LLM (OpenAI, Claude, Gemini ou outra);
  - Mantenha **memÃ³ria da conversa** por usuÃ¡rio (Redis, Postgres ou arquivo local);
  - Gere e retorne a resposta com metadados.

- **SaÃ­da:**
  ```json
  {
    "response": "As vendas do mÃªs passado foram 42 contratos fechados.",
    "model": "gpt-4o",
    "tokens_used": 823,
    "context_size": 6,
    "memory": [...]
  }
  ```

#### 2. Agente com ferramentas

Implemente uma camada de â€œAgentâ€ que decide, com base na pergunta:

- Se deve responder direto com o LLM;
- Ou acionar uma ferramenta externa (ex: buscar dados locais).

VocÃª pode usar **CrewAI, LangChain, LlamaIndex, Semantic Kernel** ou criar algo do zero.

#### 3. Observabilidade

- Registre logs estruturados contendo:
  - `user_id`
  - `prompt`
  - `tokens_used`
  - `time_ms`
  - `tool_called`

Armazene localmente (arquivo, DB ou log estruturado no console).

---

## ğŸ§® Parte 3 â€” BÃ´nus de Arquitetura Real

Esses itens sÃ£o opcionais, mas contam pontos extras:

1. **Fila de Processamento (RabbitMQ, Kafka ou BullMQ)**

   - As requisiÃ§Ãµes de IA vÃ£o para uma fila â†’ worker processa.

2. **Cache Inteligente (Redis)**

   - Se a mesma pergunta for feita novamente, retorne o resultado anterior.

3. **MÃ³dulo de MÃ©tricas**

   - Tokens por usuÃ¡rio, custo estimado, tempo mÃ©dio de resposta.

4. **ConfiguraÃ§Ã£o Multi-LLM**

   - Suporte a mÃºltiplos provedores (OpenAI, Claude, Gemini) via variÃ¡vel de ambiente.

5. **Deploy com Docker Compose**
   - Incluindo `api`, `redis` e `worker`.

---

## ğŸ§¾ EntregÃ¡veis

| Item                                 | ObrigatÃ³rio | DescriÃ§Ã£o                                    |
| ------------------------------------ | ----------- | -------------------------------------------- |
| `ARCHITECTURE.md`                    | âœ…          | Documento tÃ©cnico com visÃ£o e justificativas |
| CÃ³digo fonte                         | âœ…          | API funcional e organizada                   |
| `README.md`                          | âœ…          | InstruÃ§Ãµes de setup, execuÃ§Ã£o e endpoints    |
| Dockerfile / docker-compose          | âœ…          | ConfiguraÃ§Ã£o de ambiente                     |
| Logs e mÃ©tricas bÃ¡sicas              | âœ…          | DemonstraÃ§Ã£o de observabilidade              |
| DemonstraÃ§Ã£o (vÃ­deo ou GIF opcional) | ğŸ’¡          | Mostrando chat e ferramentas em aÃ§Ã£o         |

---

## ğŸ§© AvaliaÃ§Ã£o

| CritÃ©rio                                | Peso |
| --------------------------------------- | ---- |
| Clareza e coerÃªncia da arquitetura      | 25%  |
| CÃ³digo limpo, modular e funcional       | 25%  |
| DomÃ­nio de LLMs e frameworks de agentes | 25%  |
| Observabilidade e escalabilidade        | 15%  |
| DocumentaÃ§Ã£o tÃ©cnica                    | 10%  |

---

## ğŸ’¬ Dica Final

Mais importante do que â€œfazer funcionarâ€ Ã© **mostrar que vocÃª pensa como um arquiteto de produto de IA**.  
Queremos ver raciocÃ­nio, clareza e capacidade de projetar algo escalÃ¡vel e sustentÃ¡vel â€” nÃ£o apenas chamadas de API.

---

### ğŸ“¦ Entrega

1. Publique o repositÃ³rio em modo pÃºblico no GitHub;
2. Inclua instruÃ§Ãµes claras de como rodar (`npm install`, `docker-compose up`, etc.);
3. Envie o link do repositÃ³rio e, se quiser, um pequeno vÃ­deo de demonstraÃ§Ã£o do fluxo em aÃ§Ã£o.
